<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  

  
  <title>hadoop spark cluster setup | No pains,no gains</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="伪分布式搭建主要为了学习spark，用的3.2.1版本，所以简单搭建了伪分布式，官方文档有几个坑,首先把三台机器的hostname和ip配置下，有些case会hostname访问123192.168.77.130  k8s-master192.168.77.131  node1192.168.77.132  node2 这个一般立即生效的 参考官网修改配置文件https://hadoop.apa">
<meta property="og:type" content="article">
<meta property="og:title" content="hadoop spark cluster setup">
<meta property="og:url" content="https://mengxu2018.github.io/2019/06/03/hadoop-spark-cluster-setup/index.html">
<meta property="og:site_name" content="No pains,no gains">
<meta property="og:description" content="伪分布式搭建主要为了学习spark，用的3.2.1版本，所以简单搭建了伪分布式，官方文档有几个坑,首先把三台机器的hostname和ip配置下，有些case会hostname访问123192.168.77.130  k8s-master192.168.77.131  node1192.168.77.132  node2 这个一般立即生效的 参考官网修改配置文件https://hadoop.apa">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://mengxu2018.github.io/images/spark-cluster.png">
<meta property="og:image" content="https://mengxu2018.github.io/images/yarn.png">
<meta property="og:updated_time" content="2019-06-13T13:57:14.311Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="hadoop spark cluster setup">
<meta name="twitter:description" content="伪分布式搭建主要为了学习spark，用的3.2.1版本，所以简单搭建了伪分布式，官方文档有几个坑,首先把三台机器的hostname和ip配置下，有些case会hostname访问123192.168.77.130  k8s-master192.168.77.131  node1192.168.77.132  node2 这个一般立即生效的 参考官网修改配置文件https://hadoop.apa">
<meta name="twitter:image" content="https://mengxu2018.github.io/images/spark-cluster.png">
  
    <link rel="alternate" href="/atom.xml" title="No pains,no gains" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">No pains,no gains</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://mengxu2018.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-hadoop-spark-cluster-setup" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/06/03/hadoop-spark-cluster-setup/" class="article-date">
  <time datetime="2019-06-03T14:51:52.000Z" itemprop="datePublished">2019-06-03</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      hadoop spark cluster setup
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="/images/spark-cluster.png" alt="spark architecture"><br><img src="/images/yarn.png" alt="yarn architecture"></p>
<h2 id="伪分布式搭建"><a href="#伪分布式搭建" class="headerlink" title="伪分布式搭建"></a>伪分布式搭建</h2><p>主要为了学习spark，用的3.2.1版本，所以简单搭建了伪分布式，官方文档有几个坑,<br>首先把三台机器的hostname和ip配置下，有些case会hostname访问<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">192.168.77.130  k8s-master</span><br><span class="line">192.168.77.131  node1</span><br><span class="line">192.168.77.132  node2</span><br></pre></td></tr></table></figure></p>
<p>这个一般立即生效的</p>
<h2 id="参考官网修改配置文件"><a href="#参考官网修改配置文件" class="headerlink" title="参考官网修改配置文件"></a>参考官网修改配置文件</h2><p><a href="https://hadoop.apache.org/docs/r3.1.2/hadoop-project-dist/hadoop-common/SingleCluster.html#Pseudo-Distributed_Operation" target="_blank" rel="noopener">https://hadoop.apache.org/docs/r3.1.2/hadoop-project-dist/hadoop-common/SingleCluster.html#Pseudo-Distributed_Operation</a><br>关于官网说的这两个文件，etc/hadoop/core-site.xml, etc/hadoop/hdfs-site.xml<br><code>core-site.xml</code>比官网多了hadoop.tmp.dir, 同时用k8s-master或者ip比较好，localhost会导致远程连接失败<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">      &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;/root/hadoop-3.1.2/tmp&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://k8s-master:9000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></p>
<p><code>hdfs-site.xml</code>,比官网多了dfs.datanode.data.dir，dfs.datanode.data.dir<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"> &lt;configuration&gt;</span><br><span class="line">   &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/root/hadoop-3.1.2/dfs/name&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">   &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/root/hadoop-3.1.2/dfs/data&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">   &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.permissions&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">   &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></p>
<p>多这个几个配置目录是在启动出错的时候可以方便手动删除目录里的文件，当然提前要创建好这个三个目录</p>
<h2 id="更新启动和停止脚本"><a href="#更新启动和停止脚本" class="headerlink" title="更新启动和停止脚本"></a>更新启动和停止脚本</h2><p><code>sbin/start-dfs.sh</code>, <code>sbin/stop-dfs.sh</code>分别添加如下<br>export HDFS_NAMENODE_USER=root<br>export HDFS_DATANODE_USER=root<br>export HDFS_SECONDARYNAMENODE_USER=root<br>export YARN_RESOURCEMANAGER_USER=root<br>export YARN_NODEMANAGER_USER=root</p>
<p>其实应该有更好的地方，比如dfs-env.sh类似的被这两个脚本call的地方添加一次就够了<br>比如可以在hadoop-3.1.2/etc/hadoop/hadoop-env.sh统一设置一次</p>
<h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs namenode -format</span><br><span class="line">sbin/start-dfs.sh</span><br></pre></td></tr></table></figure>
<p>其实还可以启动./sbin/start-all来同时启动hdfs和yarn，当然这里只是搭建了一个假的分布式<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master hadoop-3.1.2]# sbin/start-</span><br><span class="line">start-all.cmd        start-balancer.sh    start-dfs.sh         start-yarn.cmd</span><br><span class="line">start-all.sh         start-dfs.cmd        start-secure-dns.sh  start-yarn.sh</span><br><span class="line">[root@k8s-master hadoop-3.1.2]# sbin/start-all.sh</span><br><span class="line">Starting namenodes on [k8s-master]</span><br><span class="line">Last login: Thu Jun 13 21:16:03 CST 2019 from 192.168.77.3 on pts/0</span><br><span class="line">Starting datanodes</span><br><span class="line">Last login: Thu Jun 13 21:16:14 CST 2019 on pts/0</span><br><span class="line">Starting secondary namenodes [k8s-master]</span><br><span class="line">Last login: Thu Jun 13 21:16:17 CST 2019 on pts/0</span><br><span class="line">Starting resourcemanager</span><br><span class="line">Last login: Thu Jun 13 21:16:25 CST 2019 on pts/0</span><br><span class="line">Starting nodemanagers</span><br><span class="line">Last login: Thu Jun 13 21:16:30 CST 2019 on pts/0</span><br></pre></td></tr></table></figure></p>
<h2 id="test"><a href="#test" class="headerlink" title="test"></a>test</h2><p>hdfs ui: <a href="http://k8s-master:9870" target="_blank" rel="noopener">http://k8s-master:9870</a><br>yarn ui: <a href="http://k8s-master:8088" target="_blank" rel="noopener">http://k8s-master:8088</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bin/hdfs dfs -mkdir /user</span><br><span class="line">bin/hdfs dfs -mkdir /user/xuhang</span><br><span class="line">bin/hdfs dfs -put etc/hadoop/*.xml /user/xuhang</span><br></pre></td></tr></table></figure>
<h2 id="部署mapreduce"><a href="#部署mapreduce" class="headerlink" title="部署mapreduce"></a>部署mapreduce</h2><p><a href="https://hadoop.apache.org/docs/r3.1.2/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html" target="_blank" rel="noopener">https://hadoop.apache.org/docs/r3.1.2/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html</a><br>可以参考这个创建一个java的mapreduce然后编译打包，跑yarn上面</p>
<h2 id="start-spark-cluster"><a href="#start-spark-cluster" class="headerlink" title="start spark cluster"></a>start spark cluster</h2><p><a href="https://spark.apache.org/docs/latest/spark-standalone.html#starting-a-cluster-manually" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/spark-standalone.html#starting-a-cluster-manually</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">在master机器，k8s-master</span><br><span class="line">./sbin/start-master.sh</span><br><span class="line"></span><br><span class="line">在其他机器</span><br><span class="line">./sbin/start-slave.sh spark://k8s-master:7077</span><br></pre></td></tr></table></figure></p>
<p>这里的spark不是运行在yarn上面，只是用了standalone的cluster，待会从hdfs取数据</p>
<h2 id="spark-ui"><a href="#spark-ui" class="headerlink" title="spark ui"></a>spark ui</h2><p><a href="http://k8s-master:8080/" target="_blank" rel="noopener">http://k8s-master:8080/</a><br>driver， work nodes</p>
<p><a href="http://k8s-master:4040" target="_blank" rel="noopener">http://k8s-master:4040</a><br>monitor ui 在job运行过程可以访问,如果想事后访问请参考history log章节</p>
<h2 id="simple-spark-code"><a href="#simple-spark-code" class="headerlink" title="simple spark code"></a>simple spark code</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">package org.apache.spark.examples</span><br><span class="line"></span><br><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line">object SimpleApp &#123;</span><br><span class="line">  def main(args: Array[String]) &#123;</span><br><span class="line">    val logFile = &quot;hdfs://k8s-master:9000/user/xuhang/core-site.xml&quot; // Should be some file on your system</span><br><span class="line">    val spark = SparkSession.builder.appName(&quot;Simple Application&quot;).getOrCreate()</span><br><span class="line">    val logData = spark.read.textFile(logFile)</span><br><span class="line">    val numAs = logData.filter(line =&gt; line.contains(&quot;a&quot;)).count()</span><br><span class="line">    val numBs = logData.filter(line =&gt; line.contains(&quot;b&quot;)).count()</span><br><span class="line">    println(s&quot;Lines with a: $numAs, Lines with b: $numBs&quot;)</span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>java version<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">package org.apache.spark.examples;/* SimpleApp.java */</span><br><span class="line">import org.apache.spark.api.java.function.FilterFunction;</span><br><span class="line">import org.apache.spark.sql.SparkSession;</span><br><span class="line">import org.apache.spark.sql.Dataset;</span><br><span class="line"></span><br><span class="line">public class SimpleApp &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        if (args.length &lt; 1) &#123;</span><br><span class="line">            System.err.println(&quot;Usage: JavaWordCount &lt;file&gt;&quot;);</span><br><span class="line">            System.exit(1);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        SparkSession spark = SparkSession.builder().appName(&quot;Simple Application&quot;).getOrCreate();</span><br><span class="line">        Dataset&lt;String&gt; logData = spark.read().textFile(args[0]).cache();</span><br><span class="line">        logData.show();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        FilterFunction&lt;String&gt; f1 = s -&gt; s.contains(&quot;a&quot;);</span><br><span class="line">        FilterFunction&lt;String&gt; f2 = s -&gt; s.contains(&quot;b&quot;);</span><br><span class="line"></span><br><span class="line">        Dataset&lt;String&gt; newDs = logData.filter(f1);</span><br><span class="line">        System.out.println(&quot;start to print new ds&quot;);</span><br><span class="line">        newDs.show();</span><br><span class="line">        System.out.println(&quot;stop to print new ds&quot;);</span><br><span class="line">        long numAs = newDs.count();</span><br><span class="line">        long numBs = logData.filter(f2).count();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        System.out.println(&quot;my Lines with a: &quot; + numAs + &quot;, lines with b: &quot; + numBs);</span><br><span class="line"></span><br><span class="line">        spark.stop();</span><br><span class="line">        System.out.println(&quot;hello world&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="saprk-history-log"><a href="#saprk-history-log" class="headerlink" title="saprk history log"></a>saprk history log</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"> mkdir /tmp/spark-events(by default)</span><br><span class="line"></span><br><span class="line"> ./bin/spark-submit   --class org.apache.spark.examples.SimpleApp</span><br><span class="line">                      --master spark://192.168.77.130:7077</span><br><span class="line">                      --deploy-mode client</span><br><span class="line">                      --executor-memory 700M</span><br><span class="line">                      --conf spark.eventLog.enabled=true  (enable history log)</span><br><span class="line">                      /root/spark/spark-example-java-1.0.jar</span><br><span class="line">                      hdfs://k8s-master:9000/user/xuhang/core-site.xml;</span><br><span class="line">./sbin/start-history-server.sh</span><br></pre></td></tr></table></figure>
<p><code>http://k8s-master:18080</code> by default<br>ref: <a href="https://spark.apache.org/docs/latest/monitoring.html" target="_blank" rel="noopener">https://spark.apache.org/docs/latest/monitoring.html</a></p>
<h2 id="Yarn的组件及架构"><a href="#Yarn的组件及架构" class="headerlink" title="Yarn的组件及架构"></a>Yarn的组件及架构</h2><ul>
<li>ResourceManager：Global（全局）的进程</li>
<li>NodeManager：运行在每个节点上的进程</li>
<li>ApplicationMaster：Application-specific（应用级别）的进程</li>
<li>Scheduler：是ResourceManager的一个组件</li>
<li>Container：节点上一组CPU和内存资源<br>Container是Yarn对计算机计算资源的抽象，它其实就是一组CPU和内存资源，所有的应用都会运行在Container中。ApplicationMaster是对运行在Yarn中某个应用的抽象，它其实就是某个类型应用的实例，ApplicationMaster是应用级别的，它的主要功能就是向ResourceManager（全局的）申请计算资源（Containers）并且和NodeManager交互来执行和监控具体的task。Scheduler是ResourceManager专门进行资源管理的一个组件，负责分配NodeManager上的Container资源，NodeManager也会不断发送自己Container使用情况给ResourceManager。<br>ResourceManager和NodeManager两个进程主要负责系统管理方面的任务。ResourceManager有一个Scheduler，负责各个集群中应用的资源分配。对于每种类型的每个应用，都会对应一个ApplicationMaster实例，ApplicationMaster通过和ResourceManager沟通获得Container资源来运行具体的job，并跟踪这个job的运行状态、监控运行进度。<br>因为application master需要向ResourceManager（全局的）申请计算资源（Containers），他应该是跑在每个node manager上面<br><a href="https://mapr.com/docs/60/MapROverview/c_application_master.html" target="_blank" rel="noopener">https://mapr.com/docs/60/MapROverview/c_application_master.html</a><br><a href="https://blog.csdn.net/suifeng3051/article/details/49486927（这个文章讲的很好" target="_blank" rel="noopener">https://blog.csdn.net/suifeng3051/article/details/49486927（这个文章讲的很好</a>)</li>
</ul>
<h2 id="总结一下"><a href="#总结一下" class="headerlink" title="总结一下"></a>总结一下</h2><p>关于Hadoop本身，有namenode, datanode, resource manager, node manager四个节点，<br>关于这个4个节点，可以搭建真正的分布式测试下，同时搞起zookeeper防止单点故障<br>namenode, datanode是hdfs，resource manager, node manager是yarn</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://mengxu2018.github.io/2019/06/03/hadoop-spark-cluster-setup/" data-id="cjwaklfgb0000igji6gn8gs3u" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2019/06/12/hive-vs-hbase/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          hive vs hbase
        
      </div>
    </a>
  
  
    <a href="/2019/06/03/create-large-file/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">随机生成大文件</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">June 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/04/">April 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/06/16/postgres安装/">postgres安装</a>
          </li>
        
          <li>
            <a href="/2019/06/13/Hadoop早期Architecture/">old Hadoop Architecture</a>
          </li>
        
          <li>
            <a href="/2019/06/13/ibm小型机/">IBM 的 POWER 处理器比较 X86 处理器</a>
          </li>
        
          <li>
            <a href="/2019/06/12/windows10-disable-update/">win10 disable auto-update</a>
          </li>
        
          <li>
            <a href="/2019/06/12/git-usage/">git 分支介绍</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 xu, hang<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>